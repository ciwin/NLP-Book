# Some mathematical definitions

*Christoph Windheuser, April 24, 2021*

## Probability and Information Theory

*(Goodfellow et.al. Deep Learning Book, p. 53 ff.)*

### Random Variables

*(Goodfellow et.al. Deep Learning Book, p. 56)*

A *random variable* is a variable $x$, that can take different values randomly. Random variables can be *discretre* or *continuous*. We only describe discrete random variables here. Discrete random variables have an finite or countably infinite number of states. 

Throwing a dice is a discrete random variable with 6 different states. 

Random variables can also be vectors $\textbf x$ consisting of several variables $x$.

Random variables only describes the states that are possible. Usually random variables are coupled with a *probability distribution* that describes the probability of each state of the random variable.

### Probability Distributions

*(Goodfellow et.al. Deep Learning Book, p. 56)*

A *probability distribution* $P(x)$ of a random variable $x$ is a description of the probability that a random variable takes each of its possible states. A probability distribution $P(x)$ must satisfy the following properties:

* For all states $x$ $P(x)$ must be greater or equal to 0 and smaller or equal to 1. If $P(x) = 0$ the state $x$ is *impossible*. If $P(x) = 1$, the state $x$ is a *sure event*, guaranteed to happen.
* The sum of $P(x)$ over all states must be 1: $\sum_{x \in x} P(x) = 1$. 

In case of a fair dice, the probability distribution is $\frac{1}{6}$ for all states:
$$
P(1) = P(2) = P(3) = P(4) = P(5) = P(6) = \frac{1}{6}
$$
The sum of $P(x)$ over all states is:
$$
\sum_{i=1}^{6}P(i) = 1
$$

### Expectation

*(Goodfellow et.al. Deep Learning Book, p. 60)*

If there is a discrete probability distribution $P(x)$ and a function $f(x)$, then the *expectation* or *expected value* of the function $f(x)$ with respect to $P(x)$ (this means that $x$ in $f(x)$ is generated by $P(x)$) is the *mean value* of $f(x)$:
$$
E_{x \sim P}[f(x)] = \sum _x P(x) f(x)
$$

### Self-Information

*(Goodfellow et.al. Deep Learning Book, p. 72)*

*Self-information* $I(x)$ specifies the information a discrete random event $x$ generated by $P(x)$ has. A sure event should have zero information, a seldom event should have high information. 
$$
I(x) = - \log P(x)
$$
We always use the *natural logarithm* for $\log$ here.

### Shanon Entropy

*(Goodfellow et.al. Deep Learning Book, p. 74)*

The *Shanon Entropy* $H(P)$ of an discrete random distribution $P(x)$ is the expectation of the self-information of all events $x$ generated by $P(x)$:
$$
H(P) = E_{x \sim P}[I(x)] = -E_{x \sim P}[\log P(x)]
$$

### Kullback-Leibler (KL) Divergence

*(Goodfellow et.al. Deep Learning Book, p. 74)*

The *Kullbach-Leibler (KL) Divergence* measures the difference of two separate random distributions $P(x)$ and $Q(x)$ over the same random variable $x$:
$$
D_{KL}(P||Q) = E_{x\sim P}\left[\log \frac{P(x)}{Q(x)}\right]= E_{x\sim P}[\log P(x) - \log Q(x)]
$$
The KL-Divergence is always *non-negative* and $0$ if and only if the two discrete distributions $P$ and $Q$ are the same. 

### Cross Entropy

*(Goodfellow et.al. Deep Learning Book, p. 75)*

The *Cross Entropy* $H(P,Q)$ of two discrete random distributions $P$ and $Q$ can directly be derived from the KL-Divergence and the Shanon Entropy:
$$
H(P,Q) = H(P) + D_{KL}(P||Q) = -E_{x \sim P}[\log Q(x)]
$$
Minimizing the cross entropy $H(P,Q)$ with respect to $Q$ is equivalent to minimizing the KL-divergence $D_{KL}(P||Q)$, because $Q$ does not participate in the omitted term.

# Gradient Descent

Learning in a multilayer perceptron is optimizing the parameters (*weights*) in a way to minimize a *cost function*  (also called *objective function, loss function* or *error function*).

As usually the optimal values of the weights cannot be calculated directly, an iterative optimization approach is used. If $f(\textbf x)$ is the error function and we want to find the optimal value for $\textbf x$ so that $f(\textbf x)$ is minimal, we can use the derivative $f'(\textbf x)$ which gives us the slope at point $\textbf x$. If the slope $f'(\textbf x) > 0$, decreasing $\textbf x$ will decrease $f(\textbf x)$. If the slope $f'(\textbf x) < 0$, increasing $\textbf x$ will decrease $f(\textbf x)$. By iteratively calculating new values for $\textbf x$ with:
$$
\textbf x^{new} = \textbf x - \epsilon f'(\textbf x)
$$
we can find at least a local minimum for $f(\textbf x)$ if $\epsilon$ is small enough. $\epsilon$ is called the *learning rate* and is a positive small number (usually $\epsilon << 1$). 

As $\textbf x$ is an $n$-dimensional vector, the derivative is also a vector called the *gradient* $\nabla_{\textbf x} f(\textbf x)$. Element $i$ of the gradient is the partial derivative of $f$ with respect to $x_i$. The iterative process of formula (2) is written:
$$
\textbf x^{new} = \textbf x - \epsilon \nabla_{\textbf x} f(\textbf x)
$$
This iterative technique is called *gradient descent* and is generally attributed to *Augustin-Louis Cauchy*, who first suggested it in 1847. 

## Cost Function

In order to train the desired behavior of a feed forward network, it is important to define the right *cost function*, as the gradient descent algorithm will minimize this function.

In case of a *classification task*, the network has to assign an $n$-dimensional input vector input vector $\textbf x \in \R^n$ to a certain class $i$ of $k$ classes $i \in \{1, ..., k \}$. One possibility to achieve this is to train the network to compute a *probability distribution* over all classes $k$ for a given $\textbf x$. 

### Maximum Likelihood Estimation

*(Goodfellow et.al. Deep Learning Book, p. 131)*











